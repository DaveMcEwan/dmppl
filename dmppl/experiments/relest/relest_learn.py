#!/usr/bin/env python3

# Use an Artificial Neural Network approach to learn alternative metric
# functions using systems generated by relest.
# Dave McEwan 2019-04-09

# Tensorflow dependency is massive so use specific venv.
#   source ../../venv3.7/bin/activate
#
# To run first create a training and evaluation dataset with relest:
#   python relest.py TRAIN exportcsv             # Default 1000 systems
#   python relest.py -q TEST exportcsv           # Quick run with 20 systems
#   mv TRAIN/csv/combined.csv ./combinedTrain.csv
#   mv TEST/csv/combined.csv ./combinedTest.csv
#
# OR
# Generate datasets in parts:
#   for d in `seq 1 5`; do
#     python relest.py RUN$d exportcsv;
#   done
#   cp RUN1/csv/combined.csv datasets/combinedTest.csv;
#   cp RUN2/csv/combined.csv datasets/combinedTrain.csv;
#   awk FNR-1 RUN{3,4,5}/csv/combined.csv >> datasets/combinedTrain.csv;
#
# Then create and fit several models to compare:
#   python relest_learn.py
# Compare progress in TensorBoard:
#   tensorboard --logdir fullassist.results/tf
#   tensorboard --logdir onchip.results/tf
#   ...etc
#   for r in relest_learn.*.results; do tensorboard --logdir $r & done
#   ...Point browsers to `localhost:6006` and `localhost:6007`
# Then score and plot specific metrics:
#   python relest.py -v \
#       -L relest_learn.inputCombA.results/qsig2_qsig2_sigm \
#       -L relest_learn.inputCombZ.results/tanh8_tanh8_sigm \
#       --load-evs relest.<date>.results/ score

# This uses TensorFlow+Keras (API v2.0.0)

# Goal is to make 2 models.
# 1. GoalBetter, aiming for better metric, using all existing metrics as input.
#    Can be made to give cycle-by-cycle estimates (slightly delayed realtime).
#    fullassist_tanh8_tanh8, 18 epochs
# 2. GoalMin, minimum calculation to compete with Cov, Dep, etc
#    hard_sigmoid can be used with fx format, Only {+,-,*,>} required
# Anything on chip must use a hard sigmoid.

from __future__ import absolute_import, division, print_function

import argparse
from contextlib import redirect_stdout
import datetime
import itertools
import json
import os
import tensorflow as tf

from dmppl.base import *
from dmppl.math import isEven, isOdd
from dmppl.nd import *
from dmppl.stats import *

# Probably fragile imports only for qsig custom activation function.
from tensorflow.python.framework import constant_op
from tensorflow.python.ops import clip_ops
from tensorflow.python.ops import math_ops

__version__ = "0.0.0"

# Activations:
# hard_sigmoid  = 0 if x < -2.5, 1 if x > 2.5, (2x+5)/10 otherwise
#   Maybe hw easier to use?  0 if x < -2.0, 1 if x > 2.0, (x+2)/4 otherwise
# relu          = max(x, 0)
# sigmoid       = 1 / (1 + exp(-x)), I.E. Logistic function
# tanh          = (exp(x) - exp(-x)) / (exp(x) + exp(-x)), I.E. Hyperbolic tangent

chosenModelParams = (
# First set. Largest models most likely to succeed.
    (8, "qsig",         8, "qsig",          "sigmoid"), # qsig8_qsig8
    #(8, "hard_sigmoid", 8, "hard_sigmoid",  "sigmoid"), # hard8_hard8
    (8, "sigmoid",      8, "sigmoid",       "sigmoid"), # sigm8_sigm8
    (8, "tanh",         8, "tanh",          "sigmoid"), # tanh8_tanh8
    (8, "relu",         8, "relu",          "sigmoid"), # relu8_relu8
# Second set. Reducing number of neurons.
    (4, "sigmoid",      4, "sigmoid",       "sigmoid"), # sigm4_sigm4
    (4, "tanh",         4, "tanh",          "sigmoid"), # tanh4_tanh4
    (4, "qsig",         4, "qsig",          "sigmoid"), # qsig4_qsig4
    (4, "qsig",         0, "qsig",          "qsig"),    # qsig4_qsig0
# Third set. Minimal models most likely good for hw.
    (2, "qsig",         0, "qsig",          "sigmoid"), # qsig2_qsig0
    (2, "qsig",         2, "qsig",          "sigmoid"), # qsig2_qsig2
)
quickdbgChosenModelParams = ((4, "qsig", 2, "qsig", "sigmoid"),)

# Use sweepModelParams to sweep a selection of model parameters.
# 5*4*5*4=400 models
numbers, activations = \
    (0, 2, 4, 8, 16), \
    ("hard_sigmoid", "sigmoid", "relu", "tanh")
sweepModelParams = list(itertools.product((numbers, activations, numbers, activations, ["sigmoid"])))

chosenInputCombinations = {
# First comb. Demonstrate using all available input.
    # Throw everything we have at the problem and hope it sticks!
    "fullAssist": ["E[X]", "E[Y]", "E[X*Y]", "E[|X-Y|]", "E[X|Y]", "E[Y|X]",
                   "Cls(X,Y)", "Cos(X,Y)", "Cov(X,Y)", "Dep(X,Y)", "Tmt(X,Y)"],

# Second comb. Demonstrate deficiencies in only performance counters.
    # No assistance to NN given, just basic performance counters.
    "perfCntrs": ["E[X]", "E[Y]"],

# Third set of combs. Demonstrate case for correlation counters
    # No assistance to NN given, but more counters.
    "withIsectSymdiff": ["E[X]", "E[Y]", "E[X*Y]", "E[|X-Y|]"],
    "withIsect": ["E[X]", "E[Y]", "E[X*Y]"],
    "withSymdiff": ["E[X]", "E[Y]", "E[|X-Y|]"],

# Fourth comb. Demonstrate diminshing returns with assistence.
    "withAssist": ["E[X]", "E[Y]", "E[X*Y]", "E[|X-Y|]",
                   "Cov(X,Y)", "Dep(X,Y)", "Tmt(X,Y)"],

#    # All these can be calculated easily in hw.
#    # - E[.] from performance counters, conditionals with a ratio.
#    # - Cov(): multiply, subtract, multiply
#    # - Dep(): multiply, ratio, subtract
#    # - Tmt(): add, subtract, ratio
#    # rejected:
#    #   - Cls(): sqrt(E[|X-Y|]) is difficult
#    #   - Cos(): Two sqrt ops
#    #   - Ham(): Just the reflection of E[|X-Y|]
#    # TODO: Finalize set for "onchip"
#    "assistA": ["E[X]", "E[Y]", "E[X*Y]", "E[|X-Y|]", "E[X|Y]", "E[Y|X]",
#                "Cov(X,Y)", "Dep(X,Y)", "Tmt(X,Y)"],
#    "assistB": ["E[X]", "E[Y]", "E[X*Y]", "E[|X-Y|]", # No Cex
#                "Cov(X,Y)", "Dep(X,Y)", "Tmt(X,Y)"],
#    "assistC": ["E[X]", "E[Y]", "E[X*Y]", # No counters for E[|X-Y|]
#                "Cov(X,Y)", "Dep(X,Y)", "Tmt(X,Y)"],
}
quickdbgInputCombinations = {"quickdbg": ["E[X]", "E[Y]", "E[X*Y]"]}

defaultLogdir = "tf.results"

def getMetric(fname): # {{{
    '''Read in a JSON file created by main() and return a callable which
    with the same API as ndCov(win, x, y) et al.

    This can be fed back into relest.py for plotting
    '''

    # NumPy implementations of activation functions to ensure TF/Keras matches
    # what we think is supposed to happen.
    activationFunctions = {
        "qsig":
            lambda x: np.clip(np.add(np.multiply(x, 0.25), 0.50), 0.0, 1.0),
        "relu":
            lambda x: np.maximum(0, x),
        "sigmoid":
            lambda x: (np.exp(-1 * x) + 1)**-1,
        "tanh":
            lambda x: np.tanh(x),
    }

    with open(fname, 'r') as fd:
        metricParams = json.load(fd)

    metricName, inputNames, ws, bs, activations = \
        metricParams["name"], \
        metricParams["inputNames"], \
        [np.array(w) for w in metricParams["ws"]], \
        [np.array(b) for b in metricParams["bs"]], \
        [activationFunctions[a] for a in metricParams["activations"]]

    def fn(win, x, y, **kwargs):
        '''Estimate the correlation between x and y.

        First calculate some standard metrics, then feed those results into
        a feed-forward neural network.
        '''

        # Standard metrics
        ffnnInput_ = []
        for nm in inputNames:
            if   "E[X]" == nm:
                _x_Ex = kwargs.get("x_Ex", None)
                value = ndEx(win, x, **kwargs) if _x_Ex is None else _x_Ex
            elif "E[Y]" == nm:
                _y_Ex = kwargs.get("y_Ex", None)
                value = ndEx(win, y, **kwargs) if _y_Ex is None else _y_Ex
            elif "E[X*Y]" == nm:
                _xHadpY_Ex = kwargs.get("xHadpY_Ex", None)
                value = ndEx(win, ndHadp(x, y, **kwargs), **kwargs) if _xHadpY_Ex is None else _xHadpY_Ex
            elif "E[|X-Y|]" == nm:
                _xDiffY_Ex = kwargs.get("xDiffY_Ex", None)
                value = ndEx(win, ndAbsDiff(x, y, **kwargs), **kwargs) if _xDiffY_Ex is None else _xDiffY_Ex
            elif "E[X|Y]" == nm:    value = ndCex(win, x, y, **kwargs)
            elif "E[Y|X]" == nm:    value = ndCex(win, y, x, **kwargs)
            elif "Cls(X,Y)" == nm:  value = ndCls(win, x, y, **kwargs)
            elif "Cos(X,Y)" == nm:  value = ndCos(win, x, y, **kwargs)
            elif "Cov(X,Y)" == nm:  value = ndCov(win, x, y, **kwargs)
            elif "Dep(X,Y)" == nm:  value = ndDep(win, x, y, **kwargs)
            elif "Ham(X,Y)" == nm:  value = ndHam(win, x, y, **kwargs)
            elif "Tmt(X,Y)" == nm:  value = ndTmt(win, x, y, **kwargs)
            else:
                assert False, nm
            ffnnInput_.append(value)

        # Feed-forward
        ret_ = np.array(ffnnInput_)
        for w, b, activation in zip(ws, bs, activations):
            ret_ = activation(np.matmul(ret_, w) + b)

        assert ret_.shape == (1,)
        return ret_[0]

    return (metricName, fn)
# }}} def getMetric

def getDatasets(**kwargs): # {{{
    # https://www.tensorflow.org/tutorials/load_data/csv

    fnameDatasetTrain = kwargs.get("fnameDatasetTrain")
    fnameDatasetTest = kwargs.get("fnameDatasetTest")
    inputCombination = kwargs.get("selectColumns", "fullassist")
    inputCombinations = kwargs.get("inputCombinations")
    selectColumns = ["known"] + inputCombinations[inputCombination]

    # NOTE: make_csv_dataset is experimental. Look out for API change.
    # NOTE: Reader is very strict! Can't cope with multi-char delimiter.

    raw_dataset_train = tf.data.experimental.make_csv_dataset(
        fnameDatasetTrain,
        batch_size=64,
        label_name="known", # NOTE: Must be in `select_columns`
        select_columns=selectColumns,
        shuffle=True,
        sloppy=True, # Sloppy --> Non-deterministic
        num_epochs=-1, # Infinitely repeating dataset
    )

    raw_dataset_test = tf.data.experimental.make_csv_dataset(
        fnameDatasetTest,
        batch_size=64,
        label_name="known", # NOTE: Must be in `select_columns`
        select_columns=selectColumns,
        shuffle=False,
        sloppy=False, # Non-sloppy --> deterministic
        num_epochs=1, # Not infinitely repeating dataset
    )

    def showBatch(batch): # {{{
        for k, v in batch.items():
            print("{:20s}: {}".format(k, v.numpy()))
    # }}} def showBatch

    def pack(features, label): # {{{
        return tf.stack(list(features.values()), axis=-1), label
    # }}} def pack

    packed_dataset_train = raw_dataset_train.map(pack)
    packed_dataset_test = raw_dataset_test.map(pack)

    # Do additional shuffling here.
    dataset_train = packed_dataset_train
    dataset_test = packed_dataset_test

    nInputs = len(selectColumns)-1

    logdir = '.'.join(("relest_learn", inputCombination, "results"))
    mkDirP(logdir)

    return nInputs, logdir, dataset_train, dataset_test
# }}} def getDatasets

def buildModel(inputCombination, **kwargs): # {{{

    def qsig(x): # {{{
        '''Hard Quarter-gradient sigmoid.

        Based on https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/backend.py#L4604-L4623
        But easy to implement in fixed point hw.
        '''
        p25 = constant_op.constant(0.25, x.dtype.base_dtype)
        p50 = constant_op.constant(0.50, x.dtype.base_dtype)
        _y0 = math_ops.mul(x, p25)
        _y1 = math_ops.add(_y0, p50)
        return clip_ops.clip_by_value(_y1, 0.0, 1.0)
    # }}} def qsig

    inputCombinations = kwargs.get("inputCombinations")
    n1, n2 = kwargs.get("n1", 8), kwargs.get("n2", 8)
    a1, a2 = kwargs.get("a1", "qsig"), kwargs.get("a2", "qsig")
    ao = kwargs.get("ao", "qsig")

    mapAct = {
        "hard_sigmoid": tf.keras.activations.hard_sigmoid,
        "sigmoid": tf.keras.activations.sigmoid,
        "relu": tf.keras.activations.relu,
        "tanh": tf.keras.activations.tanh,
        "qsig": qsig,
    }

    useHidden1, useHidden2 = (0 < n1), (0 < n2)

    modelName = "{ip}_{n1}{a1}_{n2}{a2}_{ao}".format(
        ip=inputCombination,
        a1=a1[:4] if useHidden1 else "",
        n1=n1,
        a2=a2[:4] if useHidden2 else "",
        n2=n2,
        ao=ao[:4],
    )

    nInputs = len(inputCombinations[inputCombination])
    inputs = tf.keras.Input(shape=(nInputs,))
    hidden1 = tf.keras.layers.Dense(n1, activation=mapAct[a1])(inputs)
    hidden2 = tf.keras.layers.Dense(n2, activation=mapAct[a2])(hidden1)

    # NOTE: Output activation should be smooth.
    outputs = tf.keras.layers.Dense(1, activation=mapAct[ao]) \
        (hidden2 if useHidden2 else (hidden1 if useHidden1 else inputs))

    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=modelName)

    logdir = kwargs.get("logdir", defaultLogdir)
    fnameTxt = joinP(logdir, modelName+".txt")
    fnamePdf = joinP(logdir, modelName+".pdf")

    with open(fnameTxt, 'w') as fd:
        with redirect_stdout(fd):
            model.summary()

    tf.keras.utils.plot_model(model, to_file=fnamePdf, show_shapes=True)

    return model
# }}} def buildModel

def fitModel(model, dataset, **kwargs): # {{{

    def customLoss(y_true, y_pred): # {{{
        '''Optimise against BMI.
        '''
        y_true = tf.keras.backend.cast_to_floatx(y_true)
        y_pred = tf.keras.backend.cast_to_floatx(y_pred)

        def truePositive(y_true, y_pred):
            return tf.keras.backend.sum(y_true * y_pred)

        def falsePositive(y_true, y_pred):
            return tf.keras.backend.sum((1-y_true) * y_pred)

        def falseNegative(y_true, y_pred):
            return tf.keras.backend.sum(y_true * (1-y_pred))

        def trueNegative(y_true, y_pred):
            return tf.keras.backend.sum((1-y_true) * (1-y_pred))

        tp, fp, tn, fn = \
            truePositive(y_pred, y_true), \
            falsePositive(y_pred, y_true), \
            trueNegative(y_pred, y_true), \
            falseNegative(y_pred, y_true)

        gain = bookmakersInformedness(tp, fp, fn, tn)

        return gain * -1
    # }}} def customLoss

    #fitLoss = "binary_crossentropy"
    fitLoss = customLoss

    fitOptimizer = tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=True, clipnorm=1.0, clipvalue=1.0)
    #fitOptimizer = tf.keras.optimizers.Nadam()
    #fitOptimizer = tf.keras.optimizers.Adadelta()
    #fitOptimizer = tf.keras.optimizers.SGD()

    fitMetrics = [
        "acc", # accuracy
        "mse", # mean squared error
    ]

    logdir = kwargs.get("logdir", defaultLogdir)
    tensorboardDir = joinP(logdir, "tf", model.name)

    # Run tensorboard HTTPD with:
    #   tensorboard --logdir tf.results
    fitCallbacks = [
        tf.keras.callbacks.TensorBoard(log_dir=tensorboardDir),
    ]

    model.compile(loss=fitLoss, optimizer=fitOptimizer, metrics=fitMetrics)

    fitEpochs = 100
    fitStepsPerEpoch = 100

    model.fit(dataset,
              verbose=0, # silent
              #verbose=2, # one line per epoch
              epochs=fitEpochs,
              steps_per_epoch=fitStepsPerEpoch,
              callbacks=fitCallbacks)
# }}} def fitModel

# {{{ argparser

argparser = argparse.ArgumentParser(
    description = "relest-learn - Learn FFNN-based metrics from relest dataset.",
    formatter_class = argparse.ArgumentDefaultsHelpFormatter
)

nowStr = datetime.datetime.now().strftime("%Y-%m-%d_%H:%M:%S")

argparser.add_argument("--dataset_train",
    type=str,
    default="datasets/combinedTrain.csv",
    help="Filename of training dataset CSV.")

argparser.add_argument("--dataset_test",
    type=str,
    default="datasets/combinedTest.csv",
    help="Filename of validation dataset CSV.")

argparser.add_argument("-q", "--quickdbg",
    default=False,
    action='store_true',
    help="Run with one input combination and one model.")

argparser.add_argument("--sweep",
    default=False,
    action='store_true',
    help="Sweep over model parameters.")

# }}} argparser

def main(args): # {{{


    modelParams = sweepModelParams if \
        args.sweep else (quickdbgChosenModelParams if \
        args.quickdbg else chosenModelParams)

    inputCombinations = quickdbgInputCombinations if \
        args.quickdbg else chosenInputCombinations

    for inputCombination in inputCombinations.keys():

        nInputs, logdir, dataset_train, dataset_test = \
            getDatasets(selectColumns=inputCombination,
                        inputCombinations=inputCombinations,
                        fnameDatasetTrain=args.dataset_train,
                        fnameDatasetTest=args.dataset_test)

        for i,(n1,a1,n2,a2,ao) in enumerate(modelParams):

            model = buildModel(inputCombination,
                               inputCombinations=inputCombinations,
                               n1=n1, a1=a1,
                               n2=n2, a2=a2,
                               ao=ao,
                               logdir=logdir)

            fitModel(model, dataset_train, logdir=logdir)

            loss, acc, mse = model.evaluate(dataset_test)

            fnameTxt = joinP(logdir, model.name+".txt")
            with open(fnameTxt, 'a') as fd:
                with redirect_stdout(fd):
                    print(' '.join((
                        "EVAL",
                        str(i),
                        model.name,
                        "loss=%0.04f" % loss,
                        "acc=%0.04f" % acc,
                        "mse=%0.04f" % mse,
                    )))

            weights = [a.tolist() for a in model.get_weights()]
            metricParams = {
                "name": model.name,
                "inputNames": inputCombinations[inputCombination],
                "ws": [a for i,a in enumerate(weights) if isEven(i)],
                "bs": [a for i,a in enumerate(weights) if isOdd(i)],
                "activations": [a1, a2, ao],
            }

            fnameMetric = joinP(logdir, model.name+".metric.json")
            with open(fnameMetric, 'w') as fd:
                json.dump(metricParams, fd, indent=2)

            # Give the new metric a try just for a smoke test.
            newMetricName, newMetric = getMetric(fnameMetric)
            dummy = (
                np.array([1.0, 1.0, 1.0, 1.0, 1.0]),
                np.array([0.1, 0.2, 0.3, 0.4, 0.5]),
                np.array([0.1, 0.2, 0.3, 0.4, 0.5]),
            )
            print(newMetricName, newMetric(*dummy))

#predictions = model.predict(dataset_test)
#nPred = 20
#for e, k in zip(predictions[:nPred], list(dataset_test)[0][1][:nPred]):
#    print(int(k), e[0])
#
#print('EVAL: loss={:0.03f}, acc={:0.03f}, mse={:0.03f}'.format(*model.evaluate(dataset_test)))

    return
# }}} def main

def entryPoint(argv=sys.argv):
    return run(__name__, argv=argv)

if __name__ == "__main__":
    sys.exit(entryPoint())
