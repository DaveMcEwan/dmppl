#!/usr/bin/env python3

# Use an Artificial Neural Network approach to learn alternative metric
# functions using systems generated by relest.
# Dave McEwan 2019-04-09

# To run first create a training and evaluation dataset with relest:
#   python3.7 relest.py TRAIN exportcsv             # Default 1000 systems
#   python3.7 relest.py -q TEST exportcsv           # Quick run with 20 systems
#   mv TRAIN/csv/combined.csv ./combinedTrain.csv
#   mv TEST/csv/combined.csv ./combinedTest.csv
# Then create and fit several models to compare:
#   python3.7 relest_learn.py
# Compare progress in TensorBoard:
#   tensorboard --logdir fullassist.results/tf
#   tensorboard --logdir onchip.results/tf
#   ...etc
#   ...Point browsers to `localhost:6006` and `localhost:6007`

# This uses TensorFlow+Keras (API v2.0.0)

# Goal is to make 2 models.
# 1. GoalBetter, aiming for better metric, using all existing metrics as input.
#    Can be made to give cycle-by-cycle estimates (slightly delayed realtime).
#    fullassist_tanh8_tanh8, 18 epochs
# 2. GoalMin, minimum calculation to compete with Cov, Dep, etc
#    hard_sigmoid can be used with fx format, Only {+,-,*,>} required
# Anything on chip must use a hard sigmoid.

from __future__ import absolute_import, division, print_function

import os
import tensorflow as tf
import itertools
from contextlib import redirect_stdout

from dmppl.base import *
from dmppl.stats import *

# Probably fragile imports only for qsig custom activation function.
from tensorflow.python.framework import constant_op
from tensorflow.python.ops import clip_ops
from tensorflow.python.ops import math_ops

sweepNotChosen = False

# Activations:
# hard_sigmoid  = 0 if x < -2.5, 1 if x > 2.5, (2x+5)/10 otherwise
#   Maybe hw easier to use?  0 if x < -2.0, 1 if x > 2.0, (x+2)/4 otherwise
# relu          = max(x, 0)
# sigmoid       = 1 / (1 + exp(-x)), I.E. Logistic function
# tanh          = (exp(x) - exp(-x)) / (exp(x) + exp(-x)), I.E. Hyperbolic tangent

chosenModelParams = (
    (2, "qsig",         0, "qsig"),         # qsig2_qsig0
    (2, "hard_sigmoid", 0, "hard_sigmoid"), # hard2_hard0
   #(4, "hard_sigmoid", 0, "hard_sigmoid"), # hard4_hard0
    (8, "hard_sigmoid", 0, "hard_sigmoid"), # hard8_hard0
   #(4, "hard_sigmoid", 4, "hard_sigmoid"), # hard4_hard4
    (8, "qsig",         8, "qsig"),         # qsig8_qsig8
    (8, "hard_sigmoid", 8, "hard_sigmoid"), # hard8_hard8
    (8, "sigmoid",      8, "sigmoid"),      # sigm8_sigm8
    (8, "relu",         8, "relu"),         # relu8_relu8
    (8, "tanh",         8, "tanh"),         # tanh8_tanh8
)

# Use sweepModelParams to sweep a selection of model parameters.
# 5*4*5*4=400 models
numbers, activations = \
    (0, 2, 4, 8, 16), \
    ("hard_sigmoid", "sigmoid", "relu", "tanh")
sweepModelParams = list(itertools.product((numbers, activations, numbers, activations)))

inputCombinations = {
    # Throw everything we have at the problem and hope it sticks!
    "fullassist": ["E[X]", "E[Y]", "E[X*Y]", "E[|X-Y|]", "E[X|Y]", "E[Y|X]",
                   "Cls(X,Y)", "Cos(X,Y)", "Cov(X,Y)", "Dep(X,Y)", "Ham(X,Y)",
                   "Tmt(X,Y)"],

    # No assistance to NN given, just basic performance counters.
    "basiccntrs": ["E[X]", "E[Y]"],

    # No assistance to NN given, but more performance counters.
    "morecntrs": ["E[X]", "E[Y]", "E[X*Y]", "E[|X-Y|]"],

    # All these can be calculated easily in hw.
    # - E[.] from performance counters, conditionals with a ratio.
    # - Cov(): multiply, subtract, multiply
    # - Dep(): multiply, ratio, subtract
    # - Tmt(): add, subtract, ratio
    # rejected:
    #   - Cls(): sqrt(E[|X-Y|]) is difficult
    #   - Cos(): Two sqrt ops
    #   - Ham(): Just the reflection of E[|X-Y|]
    "onchip": ["E[X]", "E[Y]", "E[X*Y]", "E[|X-Y|]", "E[X|Y]", "E[Y|X]",
               "Cov(X,Y)", "Dep(X,Y)", "Tmt(X,Y)"],
}

defaultLogdir = "tf.results"

def getDatasets(**kwargs): # {{{
    # https://www.tensorflow.org/tutorials/load_data/csv

    inputCombination = kwargs.get("selectColumns", "fullassist")
    selectColumns = ["known"] + inputCombinations[inputCombination]

    # NOTE: make_csv_dataset is experimental. Look out for API change.
    # NOTE: Reader is very strict! Can't cope with multi-char delimiter.

    raw_dataset_train = tf.data.experimental.make_csv_dataset(
        "combinedTrain.csv",
        batch_size=64,
        label_name="known", # NOTE: Must be in `select_columns`
        select_columns=selectColumns,
        shuffle=True,
        sloppy=True, # Sloppy --> Non-deterministic
        num_epochs=-1, # Infinitely repeating dataset
    )

    raw_dataset_test = tf.data.experimental.make_csv_dataset(
        "combinedTest.csv",
        batch_size=64,
        label_name="known", # NOTE: Must be in `select_columns`
        select_columns=selectColumns,
        shuffle=False,
        sloppy=False, # Non-sloppy --> deterministic
        num_epochs=1, # Not infinitely repeating dataset
    )

    def showBatch(batch): # {{{
        for k, v in batch.items():
            print("{:20s}: {}".format(k, v.numpy()))
    # }}} def showBatch

    def pack(features, label): # {{{
        return tf.stack(list(features.values()), axis=-1), label
    # }}} def pack

    packed_dataset_train = raw_dataset_train.map(pack)
    packed_dataset_test = raw_dataset_test.map(pack)

    # Do additional shuffling here.
    dataset_train = packed_dataset_train
    dataset_test = packed_dataset_test

    nInputs = len(selectColumns)-1

    logdir = '.'.join(("relest_learn", inputCombination, "results"))
    mkDirP(logdir)

    return nInputs, logdir, dataset_train, dataset_test
# }}} def getDatasets

def buildModel(nInputs, **kwargs): # {{{

    def qsig(x): # {{{
        '''Hard Quarter-gradient sigmoid.
        '''
        p25 = constant_op.constant(0.25, x.dtype.base_dtype)
        p50 = constant_op.constant(0.5, x.dtype.base_dtype)
        _y0 = math_ops.mul(x, p25)
        _y1 = math_ops.mul(_y0, p50)
        return clip_ops.clip_by_value(_y1, 0.0, 1.0)
    # }}} def qsig

    n1, n2 = kwargs.get("n1", 8), kwargs.get("n2", 8)

    a1, a2 = kwargs.get("a1", "hard_sigmoid"), kwargs.get("a2", "hard_sigmoid")
    mapAct = {
        "hard_sigmoid": tf.keras.activations.hard_sigmoid,
        "sigmoid": tf.keras.activations.sigmoid,
        "relu": tf.keras.activations.relu,
        "tanh": tf.keras.activations.tanh,
        "qsig": qsig,
    }

    useHidden1, useHidden2 = (0 < n1), (0 < n2)

    modelName = "{a1}{n1}_{a2}{n2}".format(
        a1=a1[:4] if useHidden1 else "",
        n1=n1,
        a2=a2[:4] if useHidden2 else "",
        n2=n2,
    )

    inputs = tf.keras.Input(shape=(nInputs,))
    hidden1 = tf.keras.layers.Dense(n1, activation=mapAct[a1])(inputs)
    hidden2 = tf.keras.layers.Dense(n2, activation=mapAct[a2])(hidden1)

    # NOTE: Output activation should be smooth.
    outputs = tf.keras.layers.Dense(1, activation="sigmoid") \
        (hidden2 if useHidden2 else (hidden1 if useHidden1 else inputs))

    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=modelName)

    logdir = kwargs.get("logdir", defaultLogdir)
    fnameTxt = joinP(logdir, modelName+".txt")
    fnamePng = joinP(logdir, modelName+".png")

    with open(fnameTxt, 'w') as fd:
        with redirect_stdout(fd):
            model.summary()

    tf.keras.utils.plot_model(model, to_file=fnamePng, show_shapes=True)

    return model
# }}} def buildModel

def fitModel(model, dataset, **kwargs): # {{{

    def customLoss(y_true, y_pred): # {{{
        '''Optimise against BMI.
        '''
        y_true = tf.keras.backend.cast_to_floatx(y_true)
        y_pred = tf.keras.backend.cast_to_floatx(y_pred)

        def truePositive(y_true, y_pred):
            return tf.keras.backend.sum(y_true * y_pred)

        def falsePositive(y_true, y_pred):
            return tf.keras.backend.sum((1-y_true) * y_pred)

        def falseNegative(y_true, y_pred):
            return tf.keras.backend.sum(y_true * (1-y_pred))

        def trueNegative(y_true, y_pred):
            return tf.keras.backend.sum((1-y_true) * (1-y_pred))

        tp, fp, tn, fn = \
            truePositive(y_pred, y_true), \
            falsePositive(y_pred, y_true), \
            trueNegative(y_pred, y_true), \
            falseNegative(y_pred, y_true)

        gain = bookmakersInformedness(tp, fp, fn, tn)

        return gain * -1
    # }}} def customLoss

    #fitLoss = "binary_crossentropy"
    fitLoss = customLoss

    fitOptimizer = tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=True, clipnorm=1.0, clipvalue=1.0)
    #fitOptimizer = tf.keras.optimizers.Nadam()
    #fitOptimizer = tf.keras.optimizers.Adadelta()
    #fitOptimizer = tf.keras.optimizers.SGD()

    fitMetrics = [
        "acc", # accuracy
        "mse", # mean squared error
    ]

    logdir = kwargs.get("logdir", defaultLogdir)
    tensorboardDir = joinP(logdir, "tf", model.name)

    # Run tensorboard HTTPD with:
    #   tensorboard --logdir tf.results
    fitCallbacks = [
        tf.keras.callbacks.TensorBoard(log_dir=tensorboardDir),
    ]

    model.compile(loss=fitLoss, optimizer=fitOptimizer, metrics=fitMetrics)

    fitEpochs = 100
    fitStepsPerEpoch = 100

    model.fit(dataset,
              verbose=0, # silent
              #verbose=2, # one line per epoch
              epochs=fitEpochs,
              steps_per_epoch=fitStepsPerEpoch,
              callbacks=fitCallbacks)
# }}} def fitModel

modelParams = sweepModelParams if sweepNotChosen else chosenModelParams

for inputCombination in inputCombinations.keys():

    nInputs, logdir, dataset_train, dataset_test = \
        getDatasets(selectColumns=inputCombination)

    for i,(n1,a1,n2,a2) in enumerate(modelParams):

        model = buildModel(nInputs, n1=n1, a1=a1, n2=n2, a2=a2, logdir=logdir)

        fitModel(model, dataset_train, logdir=logdir)

        loss, acc, mse = model.evaluate(dataset_test)

        fnameTxt = joinP(logdir, model.name+".txt")
        with open(fnameTxt, 'a') as fd:
            with redirect_stdout(fd):
                print(' '.join((
                    "EVAL",
                    str(i),
                    model.name,
                    "loss=%0.04f" % loss,
                    "acc=%0.04f" % acc,
                    "mse=%0.04f" % mse,
                )))

# TODO: Feed into relest and get plots like in paper.

#predictions = model.predict(dataset_test)
#nPred = 20
#for e, k in zip(predictions[:nPred], list(dataset_test)[0][1][:nPred]):
#    print(int(k), e[0])
#
#print('EVAL: loss={:0.03f}, acc={:0.03f}, mse={:0.03f}'.format(*model.evaluate(dataset_test)))
